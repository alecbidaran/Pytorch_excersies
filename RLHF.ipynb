{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQBKWcipimYKqDLygKz1VO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alecbidaran/Pytorch_excersies/blob/main/RLHF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hSjIT0_UjZj",
        "outputId": "5e66d857-48a8-4022-ce19-d9a4e5736daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'notebooks' already exists and is not an empty directory.\n",
            "/content/notebooks\n",
            "⏳ Installing base requirements ...\n",
            "✅ Base requirements installed!\n",
            "⏳ Installing Git LFS ...\n",
            "✅ Git LFS installed!\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/nlp-with-transformers/notebooks.git\n",
        "%cd notebooks\n",
        "from install import *\n",
        "install_requirements()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hide\n",
        "from utils import *\n",
        "setup_chapter()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdd37_thVHHO",
        "outputId": "d166d744-eec3-469a-c1db-1fb79f1d55be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using transformers v4.11.3\n",
            "Using datasets v1.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"gpt2\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "model=AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "V_7DD7-mVNtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "sentiment_pipeline = pipeline('text-classification',\"finiteautomata/bertweet-base-sentiment-analysis\")"
      ],
      "metadata": {
        "id": "k_OgtUV7VODg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reward(text,mode):\n",
        "  ress=sentiment_pipeline(text)\n",
        "  if mode==\"+ve\":\n",
        "    labels=torch.tensor([res['label']==\"POS\" for res in ress],dtype=torch.float32).to(device)\n",
        "  if mode==\"-ve\":\n",
        "    labels=torch.tensor([res['label']==\"NEG\" for res in ress],dtype=torch.float32).to(device)\n",
        "  scores=torch.tensor([res['score'] for res in ress],dtype=torch.float32).to(device)\n",
        "  reward=labels*scores\n",
        "  return reward\n",
        "\n"
      ],
      "metadata": {
        "id": "f-dchRm5VqTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inps = [\"I'm the king of the world!\",\n",
        "        \"I'll be back.\",\n",
        "        \"The cake is a lie\",\n",
        "        \"To be forgotten is worse than death\",\n",
        "        \"All happy families are alike; each unhappy family is unhappy in its own way.\",\n",
        "        \"You don't need a reason to help people\",\n",
        "        ]\n",
        "res = sentiment_pipeline(inps)\n",
        "\n",
        "for i in range(len(inps)):\n",
        "  res[i]['text'] = inps[i]\n",
        "  print(res[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh9qX5ctehtP",
        "outputId": "36b7902c-0329-4a7e-f3d0-f397b54c7630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'POS', 'score': 0.9771729707717896, 'text': \"I'm the king of the\n",
            "world!\"}\n",
            "{'label': 'POS', 'score': 0.5481614470481873, 'text': \"I'll be back.\"}\n",
            "{'label': 'NEG', 'score': 0.7581191658973694, 'text': 'The cake is a lie'}\n",
            "{'label': 'NEG', 'score': 0.8209365606307983, 'text': 'To be forgotten is worse\n",
            "than death'}\n",
            "{'label': 'NEU', 'score': 0.7874236702919006, 'text': 'All happy families are\n",
            "alike; each unhappy family is unhappy in its own way.'}\n",
            "{'label': 'NEU', 'score': 0.8731080889701843, 'text': \"You don't need a reason\n",
            "to help people\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_reward(inps[1], '+ve')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaEYFfUWeigo",
        "outputId": "58beb7b8-c0d4-4d04-bb56-8ba19e2b8c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5482], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "ref_model=copy.deepcopy(model)\n",
        "ref_model=ref_model.to(device)\n",
        "for param in ref_model.parameters():\n",
        "  param.requires_grad=False"
      ],
      "metadata": {
        "id": "zcNdAxSBVsTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logp_from_logits(output,labels):\n",
        "  probs=F.log_softmax(output,dim=-1)\n",
        "  return torch.gather(probs,2,labels.unsqueeze(2)).squeeze(-1)\n",
        "\n",
        "\n",
        "def generate(context,max_rate):\n",
        "    log_proba=torch.tensor([]).to(device)\n",
        "    labels=rlhf.generate(context,max_length=max_rate,do_sample=True,top_p=0.9,top_k=20)\n",
        "    att_mask=torch.ones_like(labels)\n",
        "    output = rlhf(labels,attention_mask=att_mask)\n",
        "    log_probs = logp_from_logits(\n",
        "    output.logits[:, :-1, :], labels[:, 1:])\n",
        "    log_proba=torch.cat((log_proba,log_probs),dim=1)\n",
        "    entropy=F.softmax(rlhf(context).logits,dim=-1)\n",
        "    return labels,log_proba,entropy\n",
        "\n",
        "def refrence_generate(context,max_rate):\n",
        "   log_ref_proba=torch.tensor([]).to(device)\n",
        "   with torch.no_grad():\n",
        "        ref_labels=ref_model.generate(context,max_length=max_rate,do_sample=True,top_p=0.9,top_k=20)\n",
        "        att_mask=torch.ones_like(ref_labels).to(device)\n",
        "        ref_output = ref_model(ref_labels,attention_mask=att_mask)\n",
        "   log_ref_probs = logp_from_logits(\n",
        "   ref_output.logits[:, :-1, :], ref_labels[:, 1:])\n",
        "   log_ref_proba=torch.cat((log_ref_proba,log_ref_probs),dim=1)\n",
        "   return log_ref_proba\n",
        "\n"
      ],
      "metadata": {
        "id": "vRHrPNUKV0Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "rlhf=model\n",
        "for param in rlhf.parameters():\n",
        "  param.requires_grad=True\n",
        "optimizer=torch.optim.AdamW(rlhf.parameters(),lr=1e-4,betas=(0.9,0.98),eps=1e-6)\n"
      ],
      "metadata": {
        "id": "0jMlDkqkX9AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_policy(input_txt):\n",
        "  losses=[]\n",
        "  actor_rewards=[]\n",
        "  input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "  log_ref_proba=refrence_generate(input_ids,max_rate=64)\n",
        "  seq_log_ref=torch.sum(log_ref_proba)\n",
        "  for _ in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    states,log_proba,entropy=generate(input_ids,max_rate=64)\n",
        "    seq_log=torch.sum(log_proba)\n",
        "    sentences=tokenizer.decode(states[0].tolist()).split(\".\")\n",
        "    rewards=torch.cat([get_reward(s,\"-ve\") for s in sentences]).sum()\n",
        "    ratio=(rewards-0.1*(seq_log-seq_log_ref))\n",
        "    loss_proximy=ratio*log_proba.exp().squeeze(0)\n",
        "    pol1=loss_proximy\n",
        "    pol2=torch.clamp(ratio,1-0.2,1+0.2)*log_proba.exp().squeeze(0)\n",
        "    loss=-0.5*torch.min(pol1,pol2).sum()+1e-3*entropy.mean()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    actor_rewards.append(rewards.detach().cpu().numpy())\n",
        "    losses.append(loss.item())\n",
        "  return losses,actor_rewards,states\n"
      ],
      "metadata": {
        "id": "UvPU7XQ3YeXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval_rlhf = 10\n",
        "max_iters_rlhf = 100 # start with ''The'\n",
        "input_txt = \"The\"\n",
        "count=0\n",
        "plot_loss=[]\n",
        "for iter in range(max_iters_rlhf):\n",
        "  loss,actor_rewards,states=update_policy(input_txt)\n",
        "  if iter%eval_interval_rlhf:\n",
        "    plot_loss.append(loss)\n",
        "    print('\\n')\n",
        "    print(f'loss: {np.mean(loss)}')\n",
        "    print(f'rewards:{np.mean(actor_rewards)}')\n",
        "    print(f'outputs: {tokenizer.decode(states[0].tolist())}')\n",
        "    if np.mean(actor_rewards)>0.9:\n",
        "      count+=1\n",
        "    if count==2:\n",
        "      break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDM7v-ObZz4J",
        "outputId": "bd163e6d-289b-450c-83ab-dac3b3789111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "loss: 5.523060083389282\n",
            "rewards:1.0237746238708496\n",
            "outputs: The first stage of the internet is now dominated by virtual reality\n",
            "headsets like Oculus Rift, HTC Vive, and Oculus Rift Pro. The new generation of\n",
            "gaming rigs are also using the Rift Touchpad with SteamVR to connect to virtual\n",
            "reality games with HTC Vive's SteamVR app and Oculus Touchpad controller. These\n",
            "rigs are also\n",
            "\n",
            "\n",
            "loss: -4.805917572975159\n",
            "rewards:0.5005065202713013\n",
            "outputs: The state's largest prison population was also plagued by gang\n",
            "violence, including an armed robbery.\n",
            "\n",
            "The FBI's Counter-Terrorism Center, meanwhile, is run by convicted felon Jeffrey\n",
            "Dahmer. The state prison in Chicago was home to the infamous Boston Marathon\n",
            "bombing.\n",
            "\n",
            "With the internet, cellphones and even\n",
            "\n",
            "\n",
            "loss: -7.798302173614502\n",
            "rewards:0.8612753748893738\n",
            "outputs: The first major American city to ban the LGBT community.\n",
            "\n",
            "In 2015, President Obama's former national security adviser Michael Flynn was\n",
            "accused of having ties to Russia.\n",
            "\n",
            "And while some media outlets have speculated Trump might not have the stamina to\n",
            "drive, he's clearly not afraid of alcohol.\n",
            "\n",
            "And while some\n",
            "\n",
            "\n",
            "loss: -8.044618487358093\n",
            "rewards:0.7690333724021912\n",
            "outputs: The group of white supremacist extremists who have killed hundreds of\n",
            "thousands of people, but not in Paris.\n",
            "\n",
            "\"It's time to move on,\" he told CNN's \"State of the Union.\"\n",
            "\n",
            "But some experts predict that even as they've been distracted by the internet,\n",
            "there's not quite so much reason\n",
            "\n",
            "\n",
            "loss: -7.433479833602905\n",
            "rewards:1.5682696104049683\n",
            "outputs: The main focus of the recent financial crisis.\n",
            "\n",
            "\"A lot of people were looking for a quick ride to Mexico,\" says Kevin Trenberth.\n",
            "\n",
            "If it's not getting you out of the woods, you're not going to live under the\n",
            "radar.\n",
            "\n",
            "The most common fear of driving drunk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "input_txt = \"\"\"The\n",
        "\"\"\"\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "with torch.no_grad():\n",
        "  output_temp=rlhf.generate(input_ids, max_length=128, do_sample=True,top_k=20,top_p=0.9)\n",
        "print(tokenizer.decode(output_temp[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh0SYPMOa5ds",
        "outputId": "b1eedd2b-bdb3-4372-d0c6-83163995c9f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            "\n",
            "(and other) scenarios in which you might be tempted to cheat on. But it's not so\n",
            "easy, so\n",
            "\n",
            "maybe not the world's biggest risk.\n",
            "\n",
            "As the internet's biggest user of sex, the worst of those things may be over.\n",
            "\n",
            "In case you were just getting too creative, here's how to fight against\n",
            "sex-induced stress.\n",
            "\n",
            "But there's one place where you're just making up.\n",
            "\n",
            "And it's not all that scary.\n",
            "\n",
            "In fact, there's one thing the Internet's doing that's not doing well.\n",
            "\n",
            "A little bit of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwGFVRWblDJe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}