{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "visual_trasformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNSXQN2Qm0fjp/q3FHA3/HC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alecbidaran/Pytorch_excersies/blob/main/visual_trasformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_quFQ86dd9dW",
        "outputId": "e5cb54c6-7d9c-4e1e-f48f-96fba8a794ef"
      },
      "source": [
        "!pip install torch-snippets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch-snippets\n",
            "  Downloading torch_snippets-0.426-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from torch-snippets) (0.3.4)\n",
            "Collecting loguru\n",
            "  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from torch-snippets) (5.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from torch-snippets) (3.2.2)\n",
            "Collecting rich\n",
            "  Downloading rich-10.6.0-py3-none-any.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-snippets) (4.41.1)\n",
            "Collecting fastcore\n",
            "  Downloading fastcore-1.3.20-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-snippets) (1.1.5)\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.7/dist-packages (from torch-snippets) (4.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from torch-snippets) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-snippets) (1.19.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair->torch-snippets) (2.11.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair->torch-snippets) (2.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair->torch-snippets) (0.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair->torch-snippets) (0.11.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-snippets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-snippets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-snippets) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastcore->torch-snippets) (21.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastcore->torch-snippets) (21.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->torch-snippets) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->torch-snippets) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->torch-snippets) (5.0.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->torch-snippets) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->torch-snippets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->torch-snippets) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->torch-snippets) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->torch-snippets) (57.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->torch-snippets) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->torch-snippets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair->torch-snippets) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-snippets) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-snippets) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-snippets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->torch-snippets) (0.7.0)\n",
            "Collecting colorama<0.5.0,>=0.4.0\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions<4.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from rich->torch-snippets) (3.7.4.3)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: commonmark, colorama, rich, loguru, fastcore, torch-snippets\n",
            "Successfully installed colorama-0.4.4 commonmark-0.9.1 fastcore-1.3.20 loguru-0.5.3 rich-10.6.0 torch-snippets-0.426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59AE6xS3eGxm"
      },
      "source": [
        "import torch \n",
        "from torchvision import transforms,datasets\n",
        "from torch_snippets import *\n",
        "import numpy as np \n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "import torchvision.models as models\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrpJRFFyekHH"
      },
      "source": [
        "train_transform=transforms.Compose([transforms.Resize((32,32)),\n",
        "                                  transforms.RandomRotation(0.2),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "valid_transform=transforms.Compose([\n",
        "                                    transforms.Resize((32,32)),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "test_transform=transforms.Compose([transforms.Resize((32,32)),\n",
        "                                  transforms.RandomRotation(0.4),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz2eFkvufMLa",
        "outputId": "50c860c5-26ef-442e-9094-3ff5548b1897"
      },
      "source": [
        "train_dataset=datasets.CIFAR100(root=\"./data\",download=True,train=True,transform=train_transform)\n",
        "valid_dataset=datasets.CIFAR100(root=\"./data\",download=True,train=False,transform=train_transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYNeswGAgrbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "042f3d28-6d14-4e8c-e3b6-675f8612adad"
      },
      "source": [
        "resnet=models.resnet18(pretrained=True)\n",
        "for params in resenet.parameters():\n",
        "  params.requires_grad=False\n",
        "print(resnet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf8AsAQ8f1WA"
      },
      "source": [
        "class VIT(torch.nn.Module):\n",
        "  def __init__(self,num_classes=100):\n",
        "    super(VIT,self).__init__()\n",
        "    layers=OrderedDict()\n",
        "    for name,modules in resnet.named_children():\n",
        "      if name in ['conv1','bn1','relu','maxpool','layer1',]:\n",
        "        layers[name]=modules\n",
        "    self.backbone=torch.nn.Sequential(layers)\n",
        "    #self.patch=torch.nn.Conv2d(512,256,1)\n",
        "    self.flatten=torch.nn.Flatten(2,3)\n",
        "    self.patches=torch.nn.Unfold(kernel_size=16,stride=16,padding=1)\n",
        "    self.embedding=torch.nn.Embedding((32//16)**2,768)\n",
        "    self.transformer_encoder=torch.nn.Sequential(*[torch.nn.TransformerEncoderLayer(768,12,dim_feedforward=3072,dropout=0.1) for _ in range(12)])\n",
        "    self.projection=torch.nn.Linear(768,768)\n",
        "    self.mlp=torch.nn.Sequential(torch.nn.InstanceNorm1d(768),\n",
        "                                 torch.nn.Linear(768,768),\n",
        "                                 torch.nn.GELU(),\n",
        "                                 torch.nn.Dropout(0.2),\n",
        "                                 torch.nn.Linear(768,100))\n",
        "  def forward(self,x):\n",
        "    #x=self.backbone(x)\n",
        "    x=self.patches(x).transpose(-2,-1)\n",
        "    x=self.projection(x)\n",
        "    embedding=torch.nn.Embedding(16,768,device=device)\n",
        "    pos=torch.arange(0,(32//16)**2).to(device).long()\n",
        "    pos_enc=embedding(pos.unsqueeze(0))\n",
        "    for i in range(768,2):\n",
        "      wk=0.0001**(2*i*768/16)\n",
        "      x[:][i]+=torch.sin(pos_enc.squeeze(0)*wk)\n",
        "      x[:][i+1]+=torch.cos(pos_enc.squeeze(0)*wk)\n",
        "    x1=self.transformer_encoder(x)\n",
        "    x1=self.mlp(x1)\n",
        "    return x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZP3xJZZi22f"
      },
      "source": [
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBRl-euxi9hH",
        "outputId": "c9825bc1-a2c8-4f01-c69d-38da88836516"
      },
      "source": [
        "model=VIT(num_classes=100).to(device)\n",
        "summary(model,(3,32,32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Unfold-1               [-1, 768, 4]               0\n",
            "            Linear-2               [-1, 4, 768]         590,592\n",
            "MultiheadAttention-3  [[-1, 4, 768], [-1, 2, 2]]               0\n",
            "           Dropout-4               [-1, 4, 768]               0\n",
            "         LayerNorm-5               [-1, 4, 768]           1,536\n",
            "            Linear-6              [-1, 4, 3072]       2,362,368\n",
            "           Dropout-7              [-1, 4, 3072]               0\n",
            "            Linear-8               [-1, 4, 768]       2,360,064\n",
            "           Dropout-9               [-1, 4, 768]               0\n",
            "        LayerNorm-10               [-1, 4, 768]           1,536\n",
            "TransformerEncoderLayer-11               [-1, 4, 768]               0\n",
            "MultiheadAttention-12  [[-1, 4, 768], [-1, 2, 2]]               0\n",
            "          Dropout-13               [-1, 4, 768]               0\n",
            "        LayerNorm-14               [-1, 4, 768]           1,536\n",
            "           Linear-15              [-1, 4, 3072]       2,362,368\n",
            "          Dropout-16              [-1, 4, 3072]               0\n",
            "           Linear-17               [-1, 4, 768]       2,360,064\n",
            "          Dropout-18               [-1, 4, 768]               0\n",
            "        LayerNorm-19               [-1, 4, 768]           1,536\n",
            "TransformerEncoderLayer-20               [-1, 4, 768]               0\n",
            "MultiheadAttention-21  [[-1, 4, 768], [-1, 2, 2]]               0\n",
            "          Dropout-22               [-1, 4, 768]               0\n",
            "        LayerNorm-23               [-1, 4, 768]           1,536\n",
            "           Linear-24              [-1, 4, 3072]       2,362,368\n",
            "          Dropout-25              [-1, 4, 3072]               0\n",
            "           Linear-26               [-1, 4, 768]       2,360,064\n",
            "          Dropout-27               [-1, 4, 768]               0\n",
            "        LayerNorm-28               [-1, 4, 768]           1,536\n",
            "TransformerEncoderLayer-29               [-1, 4, 768]               0\n",
            "MultiheadAttention-30  [[-1, 4, 768], [-1, 2, 2]]               0\n",
            "          Dropout-31               [-1, 4, 768]               0\n",
            "        LayerNorm-32               [-1, 4, 768]           1,536\n",
            "           Linear-33              [-1, 4, 3072]       2,362,368\n",
            "          Dropout-34              [-1, 4, 3072]               0\n",
            "           Linear-35               [-1, 4, 768]       2,360,064\n",
            "          Dropout-36               [-1, 4, 768]               0\n",
            "        LayerNorm-37               [-1, 4, 768]           1,536\n",
            "TransformerEncoderLayer-38               [-1, 4, 768]               0\n",
            "MultiheadAttention-39  [[-1, 4, 768], [-1, 2, 2]]               0\n",
            "          Dropout-40               [-1, 4, 768]               0\n",
            "        LayerNorm-41               [-1, 4, 768]           1,536\n",
            "           Linear-42              [-1, 4, 3072]       2,362,368\n",
            "          Dropout-43              [-1, 4, 3072]               0\n",
            "           Linear-44               [-1, 4, 768]       2,360,064\n",
            "          Dropout-45               [-1, 4, 768]               0\n",
            "        LayerNorm-46               [-1, 4, 768]           1,536\n",
            "TransformerEncoderLayer-47               [-1, 4, 768]               0\n",
            "MultiheadAttention-48  [[-1, 4, 768], [-1, 2, 2]]               0\n",
            "          Dropout-49               [-1, 4, 768]               0\n",
            "        LayerNorm-50               [-1, 4, 768]           1,536\n",
            "           Linear-51              [-1, 4, 3072]       2,362,368\n",
            "          Dropout-52              [-1, 4, 3072]               0\n",
            "           Linear-53               [-1, 4, 768]       2,360,064\n",
            "          Dropout-54               [-1, 4, 768]               0\n",
            "        LayerNorm-55               [-1, 4, 768]           1,536\n",
            "TransformerEncoderLayer-56               [-1, 4, 768]               0\n",
            "MultiheadAttention-57  [[-1, 4, 768], [-1, 2, 2]]               0\n",
            "          Dropout-58               [-1, 4, 768]               0\n",
            "        LayerNorm-59               [-1, 4, 768]           1,536\n",
            "           Linear-60              [-1, 4, 3072]       2,362,368\n",
            "          Dropout-61              [-1, 4, 3072]               0\n",
            "           Linear-62               [-1, 4, 768]       2,360,064\n",
            "          Dropout-63               [-1, 4, 768]               0\n",
            "        LayerNorm-64               [-1, 4, 768]           1,536\n",
            "TransformerEncoderLayer-65               [-1, 4, 768]               0\n",
            "MultiheadAttention-66  [[-1, 4, 768], [-1, 2, 2]]               0\n",
            "          Dropout-67               [-1, 4, 768]               0\n",
            "        LayerNorm-68               [-1, 4, 768]           1,536\n",
            "           Linear-69              [-1, 4, 3072]       2,362,368\n",
            "          Dropout-70              [-1, 4, 3072]               0\n",
            "           Linear-71               [-1, 4, 768]       2,360,064\n",
            "          Dropout-72               [-1, 4, 768]               0\n",
            "        LayerNorm-73               [-1, 4, 768]           1,536\n",
            "TransformerEncoderLayer-74               [-1, 4, 768]               0\n",
            "MultiheadAttention-75  [[-1, 4, 768], [-1, 2, 2]]               0\n",
            "          Dropout-76               [-1, 4, 768]               0\n",
            "        LayerNorm-77               [-1, 4, 768]           1,536\n",
            "           Linear-78              [-1, 4, 3072]       2,362,368\n",
            "          Dropout-79              [-1, 4, 3072]               0\n",
            "           Linear-80               [-1, 4, 768]       2,360,064\n",
            "          Dropout-81               [-1, 4, 768]               0\n",
            "        LayerNorm-82               [-1, 4, 768]           1,536\n",
            "TransformerEncoderLayer-83               [-1, 4, 768]               0\n",
            "MultiheadAttention-84  [[-1, 4, 768], [-1, 2, 2]]               0\n",
            "          Dropout-85               [-1, 4, 768]               0\n",
            "        LayerNorm-86               [-1, 4, 768]           1,536\n",
            "           Linear-87              [-1, 4, 3072]       2,362,368\n",
            "          Dropout-88              [-1, 4, 3072]               0\n",
            "           Linear-89               [-1, 4, 768]       2,360,064\n",
            "          Dropout-90               [-1, 4, 768]               0\n",
            "        LayerNorm-91               [-1, 4, 768]           1,536\n",
            "TransformerEncoderLayer-92               [-1, 4, 768]               0\n",
            "MultiheadAttention-93  [[-1, 4, 768], [-1, 2, 2]]               0\n",
            "          Dropout-94               [-1, 4, 768]               0\n",
            "        LayerNorm-95               [-1, 4, 768]           1,536\n",
            "           Linear-96              [-1, 4, 3072]       2,362,368\n",
            "          Dropout-97              [-1, 4, 3072]               0\n",
            "           Linear-98               [-1, 4, 768]       2,360,064\n",
            "          Dropout-99               [-1, 4, 768]               0\n",
            "       LayerNorm-100               [-1, 4, 768]           1,536\n",
            "TransformerEncoderLayer-101               [-1, 4, 768]               0\n",
            "MultiheadAttention-102  [[-1, 4, 768], [-1, 2, 2]]               0\n",
            "         Dropout-103               [-1, 4, 768]               0\n",
            "       LayerNorm-104               [-1, 4, 768]           1,536\n",
            "          Linear-105              [-1, 4, 3072]       2,362,368\n",
            "         Dropout-106              [-1, 4, 3072]               0\n",
            "          Linear-107               [-1, 4, 768]       2,360,064\n",
            "         Dropout-108               [-1, 4, 768]               0\n",
            "       LayerNorm-109               [-1, 4, 768]           1,536\n",
            "TransformerEncoderLayer-110               [-1, 4, 768]               0\n",
            "  InstanceNorm1d-111               [-1, 4, 768]               0\n",
            "          Linear-112               [-1, 4, 768]         590,592\n",
            "            GELU-113               [-1, 4, 768]               0\n",
            "         Dropout-114               [-1, 4, 768]               0\n",
            "          Linear-115               [-1, 4, 100]          76,900\n",
            "================================================================\n",
            "Total params: 57,964,132\n",
            "Trainable params: 57,964,132\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.96\n",
            "Params size (MB): 221.12\n",
            "Estimated Total Size (MB): 224.08\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta5hoJNttuIP"
      },
      "source": [
        "loss_fn=torch.nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWdddOCWyjSM"
      },
      "source": [
        "\n",
        "def train_batch(batch,model,loss_fn,optimizer):\n",
        "  image,label=batch\n",
        "  image,label=image.to(device),torch.nn.functional.one_hot(label,num_classes=100).to(device)\n",
        "  optimizer.zero_grad()\n",
        "  logit=model(image)\n",
        "  loss=loss_fn(logit,label)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  acc=(torch.max(logit,1)[1]==label).float().mean()\n",
        "  return loss.item(),acc.item()\n",
        "@torch.no_grad()\n",
        "def valid_batch(batch,model,loss_fn):\n",
        "  model.eval()\n",
        "  image,label=batch\n",
        "  image,label=image.to(device),torch.nn.functional.one_hot(label,num_classes=100).to(device)\n",
        "  logit=model(image)\n",
        "  loss=loss_fn(logit,label)\n",
        "  acc=(torch.max(logit,1)[1]==label).float().mean()\n",
        "  return loss.item(),acc.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg3gN0IlL8JD"
      },
      "source": [
        "trn_dl=torch.utils.data.DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
        "val_dl=torch.utils.data.DataLoader(valid_dataset,batch_size=64,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U2Tp-R5XMYvZ",
        "outputId": "431cd8a6-96f6-46cd-998c-eca45669338e"
      },
      "source": [
        "n_epoch=100\n",
        "log=Report(n_epoch)\n",
        "for epochs in range(n_epoch):\n",
        "  N = len(trn_dl)\n",
        "  for i,data in enumerate(trn_dl):\n",
        "    loss,acc=train_batch(data,model,loss_fn,optimizer)\n",
        "    log.record(epochs+(i+1)/N,trn_loss=loss,trn_acc=acc,end='\\r')\n",
        "  N = len(val_dl)\n",
        "  for b,data in enumerate(val_dl):\n",
        "    loss,acc=valid_batch(data,model,loss_fn)\n",
        "    log.record(epochs+(b+1)/N,val_loss=loss,val_acc=acc,end='\\r')\n",
        "  if epochs>=10:\n",
        "    optimizer=torch.optim.Adam(model.parameters(),lr=0.0001) \n",
        "  log.report_avgs(epochs+1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 1.000\ttrn_loss: 1.388\ttrn_acc: 0.250\tval_loss: 1.386\tval_acc: 0.361\t(161.19s - 15957.80s remaining)\n",
            "EPOCH: 2.000\ttrn_loss: 1.386\ttrn_acc: 0.459\tval_loss: 1.386\tval_acc: 0.544\t(320.03s - 15681.63s remaining)\n",
            "EPOCH: 3.000\ttrn_loss: 1.386\ttrn_acc: 0.545\tval_loss: 1.386\tval_acc: 0.672\t(478.91s - 15484.63s remaining)\n",
            "EPOCH: 4.000\ttrn_loss: 1.386\ttrn_acc: 0.605\tval_loss: 1.386\tval_acc: 0.560\t(637.90s - 15309.71s remaining)\n",
            "EPOCH: 5.000\ttrn_loss: 1.386\ttrn_acc: 0.647\tval_loss: 1.386\tval_acc: 0.672\t(796.71s - 15137.42s remaining)\n",
            "EPOCH: 6.000\ttrn_loss: 1.386\ttrn_acc: 0.684\tval_loss: 1.386\tval_acc: 0.684\t(955.46s - 14968.86s remaining)\n",
            "EPOCH: 7.000\ttrn_loss: 1.386\ttrn_acc: 0.700\tval_loss: 1.386\tval_acc: 0.677\t(1114.27s - 14803.83s remaining)\n",
            "EPOCH: 8.000\ttrn_loss: 1.386\ttrn_acc: 0.719\tval_loss: 1.386\tval_acc: 0.759\t(1272.97s - 14639.20s remaining)\n",
            "EPOCH: 9.000\ttrn_loss: 1.386\ttrn_acc: 0.754\tval_loss: 1.386\tval_acc: 0.821\t(1431.69s - 14475.93s remaining)\n",
            "EPOCH: 10.000\ttrn_loss: 1.386\ttrn_acc: 0.788\tval_loss: 1.386\tval_acc: 0.802\t(1590.43s - 14313.88s remaining)\n",
            "EPOCH: 11.000\ttrn_loss: 1.386\ttrn_acc: 0.809\tval_loss: 1.386\tval_acc: 0.817\t(1749.15s - 14152.20s remaining)\n",
            "EPOCH: 12.000\ttrn_loss: 1.386\ttrn_acc: 0.815\tval_loss: 1.386\tval_acc: 0.849\t(1907.84s - 13990.83s remaining)\n",
            "EPOCH: 13.000\ttrn_loss: 1.386\ttrn_acc: 0.817\tval_loss: 1.386\tval_acc: 0.758\t(2066.39s - 13828.91s remaining)\n",
            "EPOCH: 14.000\ttrn_loss: 1.386\ttrn_acc: 0.814\tval_loss: 1.386\tval_acc: 0.831\t(2225.17s - 13668.89s remaining)\n",
            "EPOCH: 15.000\ttrn_loss: 1.386\ttrn_acc: 0.814\tval_loss: 1.386\tval_acc: 0.834\t(2384.23s - 13510.65s remaining)\n",
            "EPOCH: 16.000\ttrn_loss: 1.386\ttrn_acc: 0.816\tval_loss: 1.386\tval_acc: 0.796\t(2543.13s - 13351.43s remaining)\n",
            "EPOCH: 17.000\ttrn_loss: 1.386\ttrn_acc: 0.803\tval_loss: 1.386\tval_acc: 0.761\t(2701.78s - 13191.06s remaining)\n",
            "EPOCH: 18.000\ttrn_loss: 1.386\ttrn_acc: 0.791\tval_loss: 1.386\tval_acc: 0.776\t(2860.58s - 13031.54s remaining)\n",
            "EPOCH: 19.000\ttrn_loss: 1.386\ttrn_acc: 0.789\tval_loss: 1.386\tval_acc: 0.832\t(3019.44s - 12872.34s remaining)\n",
            "EPOCH: 20.000\ttrn_loss: 1.386\ttrn_acc: 0.782\tval_loss: 1.386\tval_acc: 0.771\t(3178.43s - 12713.72s remaining)\n",
            "EPOCH: 21.000\ttrn_loss: 1.386\ttrn_acc: 0.780\tval_loss: 1.386\tval_acc: 0.752\t(3337.13s - 12553.97s remaining)\n",
            "EPOCH: 22.000\ttrn_loss: 1.386\ttrn_acc: 0.771\tval_loss: 1.386\tval_acc: 0.821\t(3496.28s - 12395.90s remaining)\n",
            "EPOCH: 23.000\ttrn_loss: 1.386\ttrn_acc: 0.772\tval_loss: 1.386\tval_acc: 0.871\t(3655.31s - 12237.36s remaining)\n",
            "EPOCH: 24.000\ttrn_loss: 1.386\ttrn_acc: 0.772\tval_loss: 1.386\tval_acc: 0.700\t(3814.55s - 12079.41s remaining)\n",
            "EPOCH: 25.000\ttrn_loss: 1.386\ttrn_acc: 0.768\tval_loss: 1.386\tval_acc: 0.700\t(3973.51s - 11920.52s remaining)\n",
            "EPOCH: 26.000\ttrn_loss: 1.386\ttrn_acc: 0.762\tval_loss: 1.386\tval_acc: 0.803\t(4132.59s - 11761.99s remaining)\n",
            "EPOCH: 27.000\ttrn_loss: 1.386\ttrn_acc: 0.747\tval_loss: 1.386\tval_acc: 0.711\t(4291.92s - 11604.07s remaining)\n",
            "EPOCH: 28.000\ttrn_loss: 1.386\ttrn_acc: 0.747\tval_loss: 1.386\tval_acc: 0.805\t(4451.07s - 11445.61s remaining)\n",
            "EPOCH: 29.000\ttrn_loss: 1.386\ttrn_acc: 0.741\tval_loss: 1.386\tval_acc: 0.719\t(4610.33s - 11287.36s remaining)\n",
            "EPOCH: 30.000\ttrn_loss: 1.386\ttrn_acc: 0.737\tval_loss: 1.386\tval_acc: 0.746\t(4769.60s - 11129.06s remaining)\n",
            "EPOCH: 31.000\ttrn_loss: 1.386\ttrn_acc: 0.737\tval_loss: 1.386\tval_acc: 0.635\t(4928.92s - 10970.82s remaining)\n",
            "EPOCH: 32.000\ttrn_loss: 1.386\ttrn_acc: 0.737\tval_loss: 1.386\tval_acc: 0.686\t(5088.08s - 10812.17s remaining)\n",
            "EPOCH: 33.000\ttrn_loss: 1.386\ttrn_acc: 0.724\tval_loss: 1.386\tval_acc: 0.680\t(5247.32s - 10653.65s remaining)\n",
            "EPOCH: 34.000\ttrn_loss: 1.386\ttrn_acc: 0.719\tval_loss: 1.386\tval_acc: 0.790\t(5406.54s - 10495.06s remaining)\n",
            "EPOCH: 35.000\ttrn_loss: 1.386\ttrn_acc: 0.726\tval_loss: 1.386\tval_acc: 0.843\t(5565.83s - 10336.55s remaining)\n",
            "EPOCH: 36.000\ttrn_loss: 1.386\ttrn_acc: 0.726\tval_loss: 1.386\tval_acc: 0.709\t(5725.17s - 10178.09s remaining)\n",
            "EPOCH: 37.000\ttrn_loss: 1.386\ttrn_acc: 0.716\tval_loss: 1.386\tval_acc: 0.714\t(5884.39s - 10019.37s remaining)\n",
            "EPOCH: 38.000\ttrn_loss: 1.386\ttrn_acc: 0.722\tval_loss: 1.386\tval_acc: 0.789\t(6043.69s - 9860.76s remaining)\n",
            "EPOCH: 39.000\ttrn_loss: 1.386\ttrn_acc: 0.728\tval_loss: 1.386\tval_acc: 0.776\t(6202.98s - 9702.09s remaining)\n",
            "EPOCH: 40.000\ttrn_loss: 1.386\ttrn_acc: 0.733\tval_loss: 1.386\tval_acc: 0.587\t(6361.78s - 9542.67s remaining)\n",
            "EPOCH: 41.000\ttrn_loss: 1.386\ttrn_acc: 0.732\tval_loss: 1.386\tval_acc: 0.739\t(6520.31s - 9382.89s remaining)\n",
            "EPOCH: 42.000\ttrn_loss: 1.386\ttrn_acc: 0.716\tval_loss: 1.386\tval_acc: 0.790\t(6678.76s - 9223.05s remaining)\n",
            "EPOCH: 42.560\ttrn_loss: 1.386\ttrn_acc: 0.492\t(6761.55s - 9125.51s remaining)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-0996e6bed56a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrn_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrn_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-aeb0043ee1bc>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(batch, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mlogit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    210\u001b[0m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl3TE-e-EU5o"
      },
      "source": [
        "log.plot_epochs(log=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KcgwY5uSu9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ec19d4-d550-4108-d047-5fc6f3599035"
      },
      "source": [
        "img,label=next(iter(val_dl))\n",
        "logits=model(img.to(device))\n",
        "_,mask=torch.max(logits.data,1)\n",
        "acc=(mask==torch.nn.functional.one_hot(label,num_classes=100).to(device)).float().mean()\n",
        "acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9900, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXXPb3ykY159"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0ZBwC8xXLTi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}